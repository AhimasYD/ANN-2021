# Практическое задание №7

Облизанов Александр, гр. 8382

Вариант 7

### Задание

Необходимо построить рекуррентную нейронную сеть, которая будет прогнозировать значение некоторого периодического сигнала.

[Здесь](https://www.google.com/url?q=https://drive.google.com/open?id%3D1vP6ik0drIk0hqiG1kYHfXv3OBuTBWHM0&sa=D&source=editors&ust=1619993292873000&usg=AOvVaw20WLA7e5ycCeWcz_AZOjIS) находится пример простой рекуррентной сети, которая предсказывает значение зашумленной синусоиды. Модель из примера не является наилучшей, а лишь демонстрирует пример построения сети со слоями GRU и LSTM.

Информация по GRU и LSTM есть в презентациях лекций. Про эти слои в Keras можно прочитать [здесь](https://www.google.com/url?q=https://keras.io/layers/recurrent/&sa=D&source=editors&ust=1619993292874000&usg=AOvVaw1rVk7By3Ql2gVEYweZPpHd).

К каждому варианту предоставляется код, который генерирует последовательность. Для выполнения задания необходимо:

1. Преобразовать последовательность в датасет, который можно подавать на вход нейронной сети (можно использовать функцию gen_data_from_sequence из примера)
2. Разбить датасет на обучающую, контрольную и тестовую выборку
3. Построить и обучить модель
4. Построить график последовательности, предсказанной на тестовой выборке (пример построения также есть в примере). Данный график необходимо также добавить в pr

Также, в файлах с кодом вариантов есть функция draw_sequence, которая позволяет нарисовать часть последовательности

### Решение

Структура модели:

* Слой GRU: 32 узла, функция активации - sigmoid, возвращает всю последовательность выходных данных для каждого элемента (return_sequences = True)
* Слой LSTM: 16 узлов, функция активации - relu, return_sequences=True, dropout = 0.25
* Слой GRU: 32 узла, reccurent_dropout = 0.25

Параметры обучения:

* Функция потерь - mean squared error
* Оптимизатор - adam
* Число эпох - 20
* Размер батча - 32

Callback's:

* EarlyStopping для остановки модели в случае переобучения. Параметр для слежения - val loss. Число эпох до остановки - 3
* ModelCheckpoint для сохранения лучшего варианта модели (наименьшая val loss)
* ReduceLROnPlateau для уменьшения скорости обучения по мере ослабления падения val loss

### Результаты

График потерь:

<img src=".\loss.png"  />

Значения после обучения:

loss: 0.0054 - val_loss: 0.0027

Сгенерированная и исходная последовательности:

<img src=".\sin.png" alt="sin" style="zoom:50%;" />


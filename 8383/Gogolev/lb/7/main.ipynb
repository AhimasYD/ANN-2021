{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ab2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "S:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPool1D, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a65f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(filename, max_review_length):\n",
    "    text = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read().lower()\n",
    "        text = (re.sub(r\"[^a-zA-Z0-9']\", \" \", text)).split()\n",
    "    dictionary = imdb.get_word_index()\n",
    "    vectorized = []\n",
    "    for word in text:\n",
    "        word = dictionary.get(word)\n",
    "        if word in range(1, 10000):\n",
    "            vectorized.append(word + 3)\n",
    "    padded = []\n",
    "    padded.append(vectorized)\n",
    "    result = sequence.pad_sequences(padded, maxlen=max_review_length)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efe05728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, x):\n",
    "    y = []\n",
    "    for model in models:\n",
    "        y.append(model.predict(x, verbose=1))\n",
    "    result = np.asarray(y)\n",
    "    return np.round(np.mean(result, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dad6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "test_size = 10000\n",
    "X_test = data[:test_size]\n",
    "Y_test = targets[:test_size]\n",
    "X_train = data[test_size:]\n",
    "Y_train = targets[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b22fd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "voc_size = 10000\n",
    "embedding_len = 32\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47c7c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(voc_size, embedding_len, input_length=max_review_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(LSTM(100, dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bb47ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(voc_size, embedding_len, input_length=max_review_length))\n",
    "    model.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c65d8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(voc_size, embedding_len, input_length=max_review_length))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10b42ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "Train on 11999 samples, validate on 1334 samples\n",
      "Epoch 1/2\n",
      "11999/11999 [==============================] - 96s 8ms/sample - loss: 0.5546 - acc: 0.6831 - val_loss: 0.3208 - val_acc: 0.8763\n",
      "Epoch 2/2\n",
      "11999/11999 [==============================] - 93s 8ms/sample - loss: 0.2946 - acc: 0.8822 - val_loss: 0.3205 - val_acc: 0.8598\n",
      "model_0 accuracy: 0.8502850532531738\n",
      "TRAINING MODEL 1\n",
      "Train on 11999 samples, validate on 1334 samples\n",
      "Epoch 1/2\n",
      "11999/11999 [==============================] - 46s 4ms/sample - loss: 0.5507 - acc: 0.6866 - val_loss: 0.3710 - val_acc: 0.8478\n",
      "Epoch 2/2\n",
      "11999/11999 [==============================] - 44s 4ms/sample - loss: 0.2461 - acc: 0.9018 - val_loss: 0.2817 - val_acc: 0.8831\n",
      "model_1 accuracy: 0.8787878751754761\n",
      "TRAINING MODEL 2\n",
      "Train on 11999 samples, validate on 1334 samples\n",
      "Epoch 1/2\n",
      "11999/11999 [==============================] - 174s 14ms/sample - loss: 0.5289 - acc: 0.7152 - val_loss: 0.3830 - val_acc: 0.8321\n",
      "Epoch 2/2\n",
      "11999/11999 [==============================] - 172s 14ms/sample - loss: 0.3175 - acc: 0.8700 - val_loss: 0.3310 - val_acc: 0.8523\n",
      "model_2 accuracy: 0.8454845547676086\n"
     ]
    }
   ],
   "source": [
    "models = [model_1(), model_2(), model_3()]\n",
    "train_size = len(X_train) // len(models)\n",
    "test_size = len(X_test) // len(models)\n",
    "for i, model in enumerate(models):\n",
    "    x_train = X_train[i * train_size: (i + 1) * train_size]\n",
    "    y_train = Y_train[i * train_size: (i + 1) * train_size]\n",
    "    x_test = X_test[i * test_size: (i + 1) * test_size]\n",
    "    y_test = Y_test[i * test_size: (i + 1) * test_size]\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"TRAINING MODEL {}\".format(i))\n",
    "    model.fit(x_train, y_train, validation_split=0.1, epochs=2, batch_size=64, verbose=1)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"model_{} accuracy: {}\".format(i, scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da9f6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 55s 5ms/sample\n",
      "10000/10000 [==============================] - 28s 3ms/sample\n",
      "10000/10000 [==============================] - 105s 11ms/sample\n",
      "Ensamble accuracy: 0.8869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ensemble_prediction = ensemble_predict(models, X_test)\n",
    "acc = accuracy_score(Y_test, ensemble_prediction)\n",
    "print(\"Ensamble accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d138061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/sample\n",
      "[[0.9295374 ]\n",
      " [0.02988201]\n",
      " [0.02072987]\n",
      " ...\n",
      " [0.9422792 ]\n",
      " [0.04105219]\n",
      " [0.03728533]]\n"
     ]
    }
   ],
   "source": [
    "print(models[0].predict(X_test, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f3eda7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 151ms/sample\n",
      "1/1 [==============================] - 0s 76ms/sample\n",
      "1/1 [==============================] - 0s 284ms/sample\n",
      "Prediction for text 1 is [[1.]]\n",
      "1/1 [==============================] - 0s 145ms/sample\n",
      "1/1 [==============================] - 0s 75ms/sample\n",
      "1/1 [==============================] - 0s 274ms/sample\n",
      "Prediction for text 2 is [[0.]]\n",
      "1/1 [==============================] - 0s 145ms/sample\n",
      "1/1 [==============================] - 0s 71ms/sample\n",
      "1/1 [==============================] - 0s 273ms/sample\n",
      "Prediction for text 3 is [[1.]]\n",
      "1/1 [==============================] - 0s 145ms/sample\n",
      "1/1 [==============================] - 0s 70ms/sample\n",
      "1/1 [==============================] - 0s 264ms/sample\n",
      "Prediction for text 4 is [[0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    text = read_from_file(str(i), max_review_length)\n",
    "    ensemble_prediction = ensemble_predict(models, text)\n",
    "    print('Prediction for text {} is {}'.format(i, ensemble_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7e374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37856be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
